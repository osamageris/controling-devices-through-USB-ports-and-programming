{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/stemosamaghandour/hull-tactical-egypt-stock-market-predictor?scriptVersionId=266408248\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","execution_count":1,"id":"10a699a5","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2025-10-07T17:56:26.258791Z","iopub.status.busy":"2025-10-07T17:56:26.258236Z","iopub.status.idle":"2025-10-07T17:56:27.611822Z","shell.execute_reply":"2025-10-07T17:56:27.61085Z"},"papermill":{"duration":1.358298,"end_time":"2025-10-07T17:56:27.613142","exception":false,"start_time":"2025-10-07T17:56:26.254844","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/input/hull-tactical-market-prediction/train.csv\n","/kaggle/input/hull-tactical-market-prediction/test.csv\n","/kaggle/input/hull-tactical-market-prediction/kaggle_evaluation/default_inference_server.py\n","/kaggle/input/hull-tactical-market-prediction/kaggle_evaluation/default_gateway.py\n","/kaggle/input/hull-tactical-market-prediction/kaggle_evaluation/__init__.py\n","/kaggle/input/hull-tactical-market-prediction/kaggle_evaluation/core/templates.py\n","/kaggle/input/hull-tactical-market-prediction/kaggle_evaluation/core/base_gateway.py\n","/kaggle/input/hull-tactical-market-prediction/kaggle_evaluation/core/relay.py\n","/kaggle/input/hull-tactical-market-prediction/kaggle_evaluation/core/kaggle_evaluation.proto\n","/kaggle/input/hull-tactical-market-prediction/kaggle_evaluation/core/__init__.py\n","/kaggle/input/hull-tactical-market-prediction/kaggle_evaluation/core/generated/kaggle_evaluation_pb2.py\n","/kaggle/input/hull-tactical-market-prediction/kaggle_evaluation/core/generated/kaggle_evaluation_pb2_grpc.py\n","/kaggle/input/hull-tactical-market-prediction/kaggle_evaluation/core/generated/__init__.py\n"]}],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"markdown","id":"484fb9a7","metadata":{"papermill":{"duration":0.001838,"end_time":"2025-10-07T17:56:27.617605","exception":false,"start_time":"2025-10-07T17:56:27.615767","status":"completed"},"tags":[]},"source":["Of course. Here is a flow chart that outlines the process for solving this stock market prediction and volatility management challenge. The chart breaks down the problem into key stages, from data preparation to model deployment for the submission file.\n","\n","```mermaid\n","flowchart TD\n","    subgraph A [Phase 1: Data Preparation & Feature Engineering]\n","        direction LR\n","        A1[Load Provided Datasets<br>Market Data & Proprietary] --> A2[Handle Missing Values & Outliers];\n","        A2 --> A3[Feature Engineering<br>Create Lagged, Rolling, &<br>Derived Features];\n","        A3 --> A4[Calculate Target Variable:<br>S&P 500 Excess Returns];\n","    end\n","\n","    subgraph B [Phase 2: Model Building & Training]\n","        B1[Define Prediction Goal<br>e.g., Next Day's Excess Return] --> B2[Select & Train<br>Machine Learning Model<br>e.g., XGBoost, LSTM, Ensemble];\n","        B2 --> B3[Tune Hyperparameters<br>with TimeSeriesSplit];\n","    end\n","\n","    subgraph C [Phase 3: Volatility-Constrained Strategy]\n","        C1[Raw Model Prediction<br>e.g., Continuous Value] --> C2{Apply Volatility Constraint<br>& Betting Logic};\n","        C2 --> C3[Allocation = 0<br>If prediction weak or volatile];\n","        C2 --> C4[Allocation = 1<br>Market Weight];\n","        C2 --> C5[Allocation = 2<br>Max Leverage<br>If prediction is strong & confident];\n","        C3 & C4 & C5 --> C6[Final Allocation Signal<br>0 to 2];\n","    end\n","\n","    subgraph D [Phase 4: Validation & Backtesting]\n","        D1[Walk-Forward Validation<br>Simulate live trading] --> D2[Calculate Evaluation Metric<br>Sharpe Ratio Variant];\n","        D2 --> D3{Performance Robust?};\n","        D3 -- No --> B2;\n","        D3 -- Yes --> D4[Final Model Ready];\n","    end\n","\n","    subgraph E [Phase 5: Submission & Inference]\n","        E1[Load Latest Data] --> E2[Generate Features];\n","        E2 --> E3[Run Trained Model];\n","        E3 --> E4[Apply Betting Strategy];\n","        E4 --> E5[Output Daily Allocation<br>to submission.csv];\n","    end\n","\n","    Start --> A\n","    A --> B\n","    B --> C\n","    C --> D\n","    D --> E\n","```\n","\n","### Step-by-Step Explanation of the Flow Chart\n","\n","**Phase 1: Data Preparation & Feature Engineering**\n","*   **Input:** You start with the provided daily data (public market info + proprietary dataset).\n","*   **Key Tasks:**\n","    *   Clean the data (handle missing values, outliers).\n","    *   Create predictive features. This is crucial. Examples include:\n","        *   **Lagged Features:** Returns from previous days (1-day, 5-day, 21-day lag).\n","        *   **Rolling Statistics:** Moving averages, rolling standard deviation (volatility), min/max over a window (e.g., 20 days).\n","        *   **Derived Features:** Ratios (e.g., P/E ratios), technical indicators (e.g., RSI, MACD), or macroeconomic trends.\n","    *   Calculate the target variable: the **excess return** of the S&P 500 (presumably over the risk-free rate).\n","\n","**Phase 2: Model Building & Training**\n","*   **Goal:** Predict the future excess return (e.g., for the next trading day).\n","*   **Process:**\n","    *   Select a machine learning model capable of capturing complex, non-linear patterns (e.g., XGBoost, LightGBM, or Neural Networks).\n","    *   Train the model on historical data, using time-series aware cross-validation (e.g., `TimeSeriesSplit`) to avoid look-ahead bias.\n","    *   Tune the model's hyperparameters to optimize predictive performance.\n","\n","**Phase 3: Volatility-Constrained Betting Strategy**\n","*   This is where you convert a raw prediction into a trade.\n","*   The model's continuous prediction needs to be mapped to an allocation between 0 and 2.\n","*   **Logic Example:**\n","    *   **Allocation = 0 (Cash):** If the predicted return is negative or below a certain confidence threshold.\n","    *   **Allocation = 1 (Market Weight):** If the prediction is mildly positive or uncertain.\n","    *   **Allocation = 2 (Max Leverage):** If the prediction is strongly positive and confident.\n","*   This logic must be designed to keep the overall strategy's volatility within the 120% constraint. A simpler version could just be `allocation = clip(prediction * scaling_factor, 0, 2)`.\n","\n","**Phase 4: Validation & Backtesting**\n","*   **Critical Step:** Test your entire pipeline (model + strategy) on historical data in a realistic way.\n","    *   Use **Walk-Forward Validation (WFV)**: Train on a period (e.g., 2000-2015), simulate trades on the next period (e.g., 2016-2018), then re-train and advance.\n","*   Calculate the competition's **Sharpe ratio variant** on your backtested results.\n","*   If performance is not robust or satisfactory, iterate back to model training or strategy design.\n","\n","**Phase 5: Submission & Inference**\n","*   Once a final model and strategy are selected, this is the production pipeline.\n","*   For each day in the test set, the process is:\n","    1.  Load the latest available data.\n","    2.  Generate the same features used in training.\n","    3.  Get the model's prediction.\n","    4.  Apply your volatility-constrained betting strategy to get the final allocation (0 to 2).\n","    5.  Output this value for the submission file (`submission.csv`)."]},{"cell_type":"code","execution_count":2,"id":"0284d96b","metadata":{"execution":{"iopub.execute_input":"2025-10-07T17:56:27.622391Z","iopub.status.busy":"2025-10-07T17:56:27.621804Z","iopub.status.idle":"2025-10-07T17:56:29.238753Z","shell.execute_reply":"2025-10-07T17:56:29.238117Z"},"papermill":{"duration":1.62066,"end_time":"2025-10-07T17:56:29.240114","exception":false,"start_time":"2025-10-07T17:56:27.619454","status":"completed"},"tags":[]},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import TimeSeriesSplit\n","from sklearn.metrics import mean_squared_error\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","execution_count":3,"id":"0c52ea0b","metadata":{"execution":{"iopub.execute_input":"2025-10-07T17:56:29.245844Z","iopub.status.busy":"2025-10-07T17:56:29.245054Z","iopub.status.idle":"2025-10-07T17:56:29.89442Z","shell.execute_reply":"2025-10-07T17:56:29.89344Z"},"papermill":{"duration":0.653593,"end_time":"2025-10-07T17:56:29.895813","exception":false,"start_time":"2025-10-07T17:56:29.24222","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Step 1: Loading data...\n","Training data shape: (8990, 98)\n","Test data shape: (10, 99)\n","\n","Training columns: ['date_id', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9', 'E1', 'E10', 'E11', 'E12', 'E13', 'E14', 'E15', 'E16', 'E17', 'E18', 'E19', 'E2', 'E20', 'E3', 'E4', 'E5', 'E6', 'E7', 'E8', 'E9', 'I1', 'I2', 'I3', 'I4', 'I5', 'I6', 'I7', 'I8', 'I9', 'M1', 'M10', 'M11', 'M12', 'M13', 'M14', 'M15', 'M16', 'M17', 'M18', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9', 'P1', 'P10', 'P11', 'P12', 'P13', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P9', 'S1', 'S10', 'S11', 'S12', 'S2', 'S3', 'S4', 'S5', 'S6', 'S7', 'S8', 'S9', 'V1', 'V10', 'V11', 'V12', 'V13', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'forward_returns', 'risk_free_rate', 'market_forward_excess_returns']\n","Test columns: ['date_id', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9', 'E1', 'E10', 'E11', 'E12', 'E13', 'E14', 'E15', 'E16', 'E17', 'E18', 'E19', 'E2', 'E20', 'E3', 'E4', 'E5', 'E6', 'E7', 'E8', 'E9', 'I1', 'I2', 'I3', 'I4', 'I5', 'I6', 'I7', 'I8', 'I9', 'M1', 'M10', 'M11', 'M12', 'M13', 'M14', 'M15', 'M16', 'M17', 'M18', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9', 'P1', 'P10', 'P11', 'P12', 'P13', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P9', 'S1', 'S10', 'S11', 'S12', 'S2', 'S3', 'S4', 'S5', 'S6', 'S7', 'S8', 'S9', 'V1', 'V10', 'V11', 'V12', 'V13', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'is_scored', 'lagged_forward_returns', 'lagged_risk_free_rate', 'lagged_market_forward_excess_returns']\n","Available price columns: []\n","Step 2: Creating features...\n","No standard price column found. Using first numeric column: 'date_id'\n","Creating features using price column: date_id\n","Number of features before handling missing values: 130\n","Number of features after validation: 122\n","Step 3: Training model...\n","Training on 8990 samples with 122 features\n","Error in main execution: Input y contains infinity or a value too large for dtype('float64').\n","Attempting to create a fallback submission...\n","Fallback submission created with market weight (1.0)\n"]}],"source":["\n","\n","class HullTacticalPredictor:\n","    def __init__(self):\n","        self.model = None\n","        self.scaler = StandardScaler()\n","        self.feature_columns = None\n","        self.volatility_lookback = 20\n","        self.allocation_threshold = 0.001\n","        \n","    def load_data(self, train_path, test_path):\n","        \"\"\"\n","        Load and merge training and test datasets\n","        \"\"\"\n","        train_df = pd.read_csv(train_path)\n","        test_df = pd.read_csv(test_path)\n","        \n","        print(f\"Training data shape: {train_df.shape}\")\n","        print(f\"Test data shape: {test_df.shape}\")\n","        \n","        # Display column names to understand the data structure\n","        print(\"\\nTraining columns:\", train_df.columns.tolist())\n","        print(\"Test columns:\", test_df.columns.tolist())\n","        \n","        # Check for common price columns\n","        price_columns = ['Close', 'close', 'Adj Close', 'Price', 'price', 'SP500', 'sp500']\n","        available_price_cols = [col for col in train_df.columns if col in price_columns]\n","        print(f\"Available price columns: {available_price_cols}\")\n","        \n","        # Combine for consistent feature engineering\n","        train_df['is_train'] = True\n","        test_df['is_train'] = False\n","        full_df = pd.concat([train_df, test_df], ignore_index=True)\n","        \n","        return full_df, train_df, test_df\n","    \n","    def find_price_column(self, df):\n","        \"\"\"\n","        Automatically find the price column in the dataset\n","        \"\"\"\n","        price_columns = ['Close', 'close', 'Adj Close', 'Price', 'price', 'SP500', 'sp500', 'value']\n","        \n","        for col in price_columns:\n","            if col in df.columns:\n","                print(f\"Using '{col}' as price column\")\n","                return col\n","        \n","        # If no standard price column found, look for numeric columns that could be prices\n","        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n","        if len(numeric_cols) > 0:\n","            print(f\"No standard price column found. Using first numeric column: '{numeric_cols[0]}'\")\n","            return numeric_cols[0]\n","        else:\n","            raise KeyError(\"No suitable price column found in the dataset\")\n","    \n","    def calculate_target(self, df):\n","        \"\"\"\n","        Calculate excess returns target variable\n","        \"\"\"\n","        # Find the price column\n","        price_col = self.find_price_column(df)\n","        \n","        # Calculate daily returns\n","        df['sp500_return'] = df[price_col].pct_change()\n","        \n","        # For simplicity, assuming risk-free rate is 0 or very small\n","        df['excess_return'] = df['sp500_return']  # - risk_free_rate\n","        \n","        # Create forward-looking target (next day's excess return)\n","        df['target'] = df['excess_return'].shift(-1)\n","        \n","        return df, price_col\n","    \n","    def create_features(self, df, price_col):\n","        \"\"\"\n","        Create technical features for prediction using the identified price column\n","        \"\"\"\n","        print(f\"Creating features using price column: {price_col}\")\n","        \n","        # Price-based features\n","        if 'Open' in df.columns:\n","            df['price_ratio'] = df[price_col] / df['Open']\n","        if 'High' in df.columns and 'Low' in df.columns:\n","            df['high_low_ratio'] = df['High'] / df['Low']\n","        if 'Open' in df.columns:\n","            df['close_open_ratio'] = df[price_col] / df['Open']\n","        \n","        # Moving averages\n","        for window in [5, 10, 20, 50]:\n","            df[f'sma_{window}'] = df[price_col].rolling(window=window).mean()\n","            df[f'ema_{window}'] = df[price_col].ewm(span=window).mean()\n","            df[f'price_vs_sma_{window}'] = df[price_col] / df[f'sma_{window}']  \n","        \n","        # Volatility features\n","        df['volatility_5'] = df['sp500_return'].rolling(window=5).std()\n","        df['volatility_20'] = df['sp500_return'].rolling(window=20).std()\n","        df['volatility_50'] = df['sp500_return'].rolling(window=50).std()\n","        \n","        # Momentum indicators\n","        df['momentum_5'] = df[price_col] / df[price_col].shift(5) - 1\n","        df['momentum_10'] = df[price_col] / df[price_col].shift(10) - 1\n","        df['momentum_20'] = df[price_col] / df[price_col].shift(20) - 1\n","        \n","        # RSI-like features\n","        for window in [5, 14]:\n","            delta = df[price_col].diff()\n","            gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n","            loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n","            rs = gain / loss\n","            df[f'rsi_{window}'] = 100 - (100 / (1 + rs))\n","        \n","        # Volume features\n","        if 'Volume' in df.columns:\n","            df['volume_sma'] = df['Volume'].rolling(window=20).mean()\n","            df['volume_ratio'] = df['Volume'] / df['volume_sma']\n","        elif 'volume' in df.columns:\n","            df['volume_sma'] = df['volume'].rolling(window=20).mean()\n","            df['volume_ratio'] = df['volume'] / df['volume_sma']\n","        \n","        # Lagged returns\n","        for lag in [1, 2, 3, 5, 10]:\n","            df[f'return_lag_{lag}'] = df['sp500_return'].shift(lag)\n","        \n","        # Rolling return statistics\n","        df['rolling_return_5'] = df['sp500_return'].rolling(window=5).mean()\n","        df['rolling_return_20'] = df['sp500_return'].rolling(window=20).mean()\n","        df['rolling_std_5'] = df['sp500_return'].rolling(window=5).std()\n","        df['rolling_std_20'] = df['sp500_return'].rolling(window=20).std()\n","        \n","        # Date-based features (if Date column exists)\n","        if 'Date' in df.columns:\n","            df['Date'] = pd.to_datetime(df['Date'])\n","            df['day_of_week'] = df['Date'].dt.dayofweek\n","            df['month'] = df['Date'].dt.month\n","            df['quarter'] = df['Date'].dt.quarter\n","        \n","        return df\n","    \n","    def prepare_features(self, df):\n","        \"\"\"\n","        Prepare final feature set and handle missing values\n","        \"\"\"\n","        # Select feature columns (exclude non-feature columns)\n","        exclude_cols = ['Date', 'date', 'target', 'sp500_return', 'excess_return', 'is_train']\n","        feature_columns = [col for col in df.columns if col not in exclude_cols and df[col].dtype in ['float64', 'int64']]\n","        \n","        print(f\"Number of features before handling missing values: {len(feature_columns)}\")\n","        \n","        # Handle missing values\n","        df[feature_columns] = df[feature_columns].fillna(method='ffill').fillna(method='bfill').fillna(0)\n","        \n","        # Remove columns with too many zeros or constant values\n","        valid_features = []\n","        for col in feature_columns:\n","            if df[col].nunique() > 1 and df[col].std() > 0:\n","                valid_features.append(col)\n","        \n","        self.feature_columns = valid_features\n","        print(f\"Number of features after validation: {len(self.feature_columns)}\")\n","        \n","        return df, self.feature_columns\n","    \n","    def train_model(self, train_df, feature_columns):\n","        \"\"\"\n","        Train the prediction model with time-series cross validation\n","        \"\"\"\n","        # Prepare training data\n","        train_mask = train_df['is_train'] & train_df['target'].notna()\n","        X_train = train_df.loc[train_mask, feature_columns]\n","        y_train = train_df.loc[train_mask, 'target']\n","        \n","        # Remove any remaining NaN values\n","        valid_mask = X_train.notna().all(axis=1) & y_train.notna()\n","        X_train = X_train[valid_mask]\n","        y_train = y_train[valid_mask]\n","        \n","        print(f\"Training on {len(X_train)} samples with {len(feature_columns)} features\")\n","        \n","        if len(X_train) == 0:\n","            raise ValueError(\"No valid training data available after preprocessing\")\n","        \n","        # Scale features\n","        X_train_scaled = self.scaler.fit_transform(X_train)\n","        \n","        # Train model (using Gradient Boosting for non-linear relationships)\n","        self.model = GradientBoostingRegressor(\n","            n_estimators=100,\n","            learning_rate=0.1,\n","            max_depth=5,\n","            min_samples_split=100,\n","            random_state=42\n","        )\n","        \n","        self.model.fit(X_train_scaled, y_train)\n","        \n","        # Feature importance\n","        feature_importance = pd.DataFrame({\n","            'feature': feature_columns,\n","            'importance': self.model.feature_importances_\n","        }).sort_values('importance', ascending=False)\n","        \n","        print(\"\\nTop 10 most important features:\")\n","        print(feature_importance.head(10))\n","        \n","        return self.model\n","    \n","    def volatility_constrained_allocation(self, predictions, recent_volatility, market_volatility):\n","        \"\"\"\n","        Apply volatility constraints to convert predictions to allocations (0-2)\n","        \"\"\"\n","        allocations = []\n","        \n","        for i, pred in enumerate(predictions):\n","            # Base allocation based on prediction strength\n","            if abs(pred) < self.allocation_threshold:\n","                # Weak signal - stay in cash (0)\n","                allocation = 0.0\n","            elif pred > 0.02:  # Strong positive prediction\n","                allocation = min(2.0, 1.0 + pred * 10)  # Scale with prediction strength\n","            elif pred > 0.005:  # Moderate positive prediction\n","                allocation = 1.0 + (pred - 0.005) * 20  # Some leverage\n","            elif pred > 0:  # Weak positive prediction\n","                allocation = 1.0  # Market weight\n","            elif pred > -0.005:  # Weak negative prediction\n","                allocation = 0.5  # Reduced exposure\n","            else:  # Strong negative prediction\n","                allocation = 0.0  # Cash\n","            \n","            # Apply volatility adjustment\n","            if i < len(recent_volatility):\n","                vol_ratio = recent_volatility[i] / market_volatility if market_volatility > 0 else 1\n","                if vol_ratio > 1.2:  # If volatility is too high relative to market\n","                    allocation = max(0, allocation * 0.5)  # Reduce position\n","            \n","            # Ensure allocation is within [0, 2] range\n","            allocation = max(0.0, min(2.0, allocation))\n","            allocations.append(allocation)\n","        \n","        return np.array(allocations)\n","    \n","    def predict_allocations(self, test_df, feature_columns):\n","        \"\"\"\n","        Generate predictions and convert to volatility-constrained allocations\n","        \"\"\"\n","        # Prepare test features\n","        X_test = test_df[feature_columns].fillna(0)\n","        \n","        if len(X_test) == 0:\n","            # Fallback: return market weight if no features available\n","            print(\"Warning: No features available for prediction. Using market weight (1.0)\")\n","            return np.ones(len(test_df)), np.zeros(len(test_df))\n","        \n","        X_test_scaled = self.scaler.transform(X_test)\n","        \n","        # Get raw predictions\n","        raw_predictions = self.model.predict(X_test_scaled)\n","        \n","        # Calculate recent volatility for constraint application\n","        if 'sp500_return' in test_df.columns:\n","            recent_volatility = test_df['sp500_return'].rolling(window=5, min_periods=1).std().fillna(0.02).values\n","            market_volatility = 0.02  # Approximate long-term market volatility\n","        else:\n","            # Fallback if return data not available\n","            recent_volatility = np.full(len(test_df), 0.02)\n","            market_volatility = 0.02\n","        \n","        # Apply volatility constraints to get final allocations\n","        allocations = self.volatility_constrained_allocation(\n","            raw_predictions, recent_volatility, market_volatility\n","        )\n","        \n","        return allocations, raw_predictions\n","    \n","    def backtest_strategy(self, train_df, feature_columns):\n","        \"\"\"\n","        Perform walk-forward validation to test strategy performance\n","        \"\"\"\n","        print(\"Running backtest...\")\n","        \n","        # Use time series split for validation\n","        tscv = TimeSeriesSplit(n_splits=3)  # Reduced for speed\n","        X = train_df[feature_columns].fillna(0)\n","        y = train_df['target']\n","        \n","        # Only use training period with valid targets\n","        train_mask = train_df['is_train'] & y.notna()\n","        X = X[train_mask]\n","        y = y[train_mask]\n","        \n","        if len(X) == 0:\n","            print(\"No data available for backtesting\")\n","            return 0\n","        \n","        performances = []\n","        \n","        for train_idx, test_idx in tscv.split(X):\n","            # Split data\n","            X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n","            y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n","            \n","            if len(X_train) == 0 or len(X_test) == 0:\n","                continue\n","            \n","            # Scale features\n","            X_train_scaled = self.scaler.fit_transform(X_train)\n","            X_test_scaled = self.scaler.transform(X_test)\n","            \n","            # Train model\n","            model = GradientBoostingRegressor(\n","                n_estimators=50,  # Smaller for faster backtesting\n","                learning_rate=0.1,\n","                max_depth=4,\n","                random_state=42\n","            )\n","            model.fit(X_train_scaled, y_train)\n","            \n","            # Predict\n","            predictions = model.predict(X_test_scaled)\n","            \n","            # Simple allocation strategy for backtest\n","            allocations = np.where(predictions > 0.005, 1.2, \n","                                 np.where(predictions < -0.005, 0.8, 1.0))\n","            \n","            # Calculate strategy returns\n","            strategy_returns = allocations * y_test.values\n","            \n","            # Calculate Sharpe ratio (simplified)\n","            sharpe = np.mean(strategy_returns) / np.std(strategy_returns) if np.std(strategy_returns) > 0 else 0\n","            \n","            performances.append(sharpe)\n","            print(f\"Fold Sharpe ratio: {sharpe:.4f}\")\n","        \n","        if performances:\n","            avg_performance = np.mean(performances)\n","            print(f\"Average backtest Sharpe ratio: {avg_performance:.4f}\")\n","            return avg_performance\n","        else:\n","            print(\"No valid backtest results\")\n","            return 0\n","    \n","    def create_submission(self, test_df, allocations):\n","        \"\"\"\n","        Create submission file in the required format\n","        \"\"\"\n","        # Find the date column\n","        date_col = 'Date' if 'Date' in test_df.columns else 'date' if 'date' in test_df.columns else test_df.columns[0]\n","        \n","        submission = pd.DataFrame({\n","            'Date': test_df[date_col],\n","            'Allocation': allocations\n","        })\n","        \n","        # Ensure allocations are within [0, 2] range\n","        submission['Allocation'] = submission['Allocation'].clip(0, 2)\n","        \n","        return submission\n","\n","def main():\n","    \"\"\"\n","    Main execution function\n","    \"\"\"\n","    # Initialize predictor\n","    predictor = HullTacticalPredictor()\n","    \n","    # File paths (update these based on your Kaggle environment)\n","    train_path = \"/kaggle/input/hull-tactical-market-prediction/train.csv\"\n","    test_path = \"/kaggle/input/hull-tactical-market-prediction/test.csv\"\n","    \n","    try:\n","        # Step 1: Load data\n","        print(\"Step 1: Loading data...\")\n","        full_df, train_df, test_df = predictor.load_data(train_path, test_path)\n","        \n","        # Step 2: Calculate target and create features\n","        print(\"Step 2: Creating features...\")\n","        full_df, price_col = predictor.calculate_target(full_df)\n","        full_df = predictor.create_features(full_df, price_col)\n","        full_df, feature_columns = predictor.prepare_features(full_df)\n","        \n","        # Step 3: Train model\n","        print(\"Step 3: Training model...\")\n","        predictor.train_model(full_df, feature_columns)\n","        \n","        # Step 4: Backtest strategy\n","        print(\"Step 4: Backtesting strategy...\")\n","        backtest_sharpe = predictor.backtest_strategy(full_df, feature_columns)\n","        \n","        # Step 5: Generate predictions for test set\n","        print(\"Step 5: Generating test predictions...\")\n","        allocations, raw_predictions = predictor.predict_allocations(test_df, feature_columns)\n","        \n","        # Step 6: Create submission\n","        print(\"Step 6: Creating submission file...\")\n","        submission = predictor.create_submission(test_df, allocations)\n","        \n","        # Display results\n","        print(\"\\n\" + \"=\"*50)\n","        print(\"PREDICTION SUMMARY\")\n","        print(\"=\"*50)\n","        print(f\"Backtest Sharpe Ratio: {backtest_sharpe:.4f}\")\n","        print(f\"Allocation Statistics:\")\n","        print(f\"  Min: {allocations.min():.4f}\")\n","        print(f\"  Max: {allocations.max():.4f}\")\n","        print(f\"  Mean: {allocations.mean():.4f}\")\n","        print(f\"  Std: {allocations.std():.4f}\")\n","        \n","        # Show allocation distribution\n","        allocation_bins = {\n","            \"Cash (0-0.2)\": ((allocations >= 0) & (allocations < 0.2)).sum(),\n","            \"Reduced (0.2-0.8)\": ((allocations >= 0.2) & (allocations < 0.8)).sum(),\n","            \"Market (0.8-1.2)\": ((allocations >= 0.8) & (allocations < 1.2)).sum(),\n","            \"Leveraged (1.2-2.0)\": ((allocations >= 1.2) & (allocations <= 2.0)).sum()\n","        }\n","        \n","        print(\"\\nAllocation Distribution:\")\n","        for category, count in allocation_bins.items():\n","            percentage = (count / len(allocations)) * 100\n","            print(f\"  {category}: {count} days ({percentage:.1f}%)\")\n","        \n","        # Save submission\n","        submission.parquet = \"submission.parquet.csv\"\n","        submission.to_csv(submission.parquet, index=False)\n","        print(f\"\\nSubmission saved to: {submission.parquet}\")\n","        #submission_file\n","        #submission.csv\n","        # Display first few rows of submission\n","        print(\"\\nFirst 10 rows of submission:\")\n","        print(submission.head(10))\n","        \n","        return submission, allocations, raw_predictions\n","        \n","    except Exception as e:\n","        print(f\"Error in main execution: {str(e)}\")\n","        print(\"Attempting to create a fallback submission...\")\n","        \n","        # Fallback: create a simple market-weight submission\n","        test_df = pd.read_csv(test_path)\n","        date_col = test_df.columns[0]\n","        fallback_submission = pd.DataFrame({\n","            'Date': test_df[date_col],\n","            'Allocation': 1.0  # Market weight\n","        })\n","        fallback_submission.to_csv(\"submission.parquet.csv\", index=False)\n","        print(\"Fallback submission created with market weight (1.0)\")\n","        return fallback_submission, np.ones(len(test_df)), np.zeros(len(test_df))\n","\n","if __name__ == \"__main__\":\n","    submission, allocations, raw_predictions = main()"]},{"cell_type":"markdown","id":"6e578891","metadata":{"papermill":{"duration":0.001831,"end_time":"2025-10-07T17:56:29.899971","exception":false,"start_time":"2025-10-07T17:56:29.89814","status":"completed"},"tags":[]},"source":["ey fixes made:\n","\n","Automatic Price Column Detection: The code now automatically finds the price column by checking common names like 'Close', 'close', 'Price', etc.\n","\n","Better Error Handling: Added comprehensive error handling and fallback mechanisms.\n","\n","Data Validation: Added checks for empty datasets and invalid features.\n","\n","Flexible Column Handling: The code adapts to whatever column names are actually in your dataset.\n","\n","Fallback Submission: If anything goes wrong, it creates a simple market-weight submission as a fallback.\n","\n","The code will now:\n","\n","Print the actual column names in your dataset\n","\n","Automatically identify which column contains price data\n","\n","Adapt the feature engineering to use the available columns\n","\n","Provide detailed debugging information\n","\n","Create a valid submission even if there are issues\n","\n"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"databundleVersionId":13750964,"sourceId":111543,"sourceType":"competition"}],"dockerImageVersionId":31153,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"papermill":{"default_parameters":{},"duration":7.584381,"end_time":"2025-10-07T17:56:30.318968","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-10-07T17:56:22.734587","version":"2.6.0"}},"nbformat":4,"nbformat_minor":5}